{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"source": [
				"### Historical and CDC loading \n",
				"###### database: Insurance\n",
				"###### table: Policy"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"%iam_role arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
				"%region us-east-1\n",
				"%idle_timeout 5\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [],
			"source": [
				"\n",
				"%%configure -f\n",
				"{\n",
				"    \"conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
				"    \"datalake-formats\":\"delta\",\n",
				"    'enable-auto-scaling': 'false',\n",
				"    'JOB_NAME': 'glue-job-policy-insurance-full-load', \n",
				"    's3_bucket': 's3://jamil-datalake-dev/',\n",
				"    'start_date': 'cron',\n",
				"    'final_date': 'cron',\n",
				"    'environment': 'prd', \n",
				"    'file_format': 'csv',\n",
				"    'reprocess_all': 'True'\n",
				"}\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"import boto3\n",
				"import pyspark.sql.functions as F\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.types import StructType, StructField, StringType\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from delta import DeltaTable\n",
				"from datetime import date, datetime, timedelta\n",
				"from dateutil.parser import parse\n",
				"from re import sub\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"def to_snake_case(text):\n",
				"    return ('_'.join(sub('([A-Z][a-z]+)', r' \\1', \n",
				"                        sub('([A-Z]+)', r' \\1', \n",
				"                            text.replace('-', ' ')))\n",
				"                    .split())\n",
				"                .lower())\n",
				"\n",
				"\n",
				"def dedup_keys_str(primary_keys: list) -> str:\n",
				"    condition_list = [f'target.{key} = delta.{key}' for key in primary_keys]\n",
				"    dedup_str = ' AND '.join(condition_list)\n",
				"    return dedup_str\n",
				"\n",
				"\n",
				"def table_exists(spark, database, table) -> bool:\n",
				"    exist = spark.sql(f\"select * from {database}.{table} limit 1\")\n",
				"    return bool(exist.count() > 0)\n",
				"\n",
				"\n",
				"def delta_table_exists(spark, path) -> bool:\n",
				"    exist = False\n",
				"    try:\n",
				"        DeltaTable.forPath(spark, path)\n",
				"        exist = True\n",
				"    except:\n",
				"        exist = False\n",
				"    return exist\n",
				"\n",
				"\n",
				"def is_valid_date(date_ymd) -> bool:\n",
				"    is_valid = False\n",
				"    if date_ymd:\n",
				"        try:\n",
				"            parse(timestr=date_ymd, yearfirst=True, dayfirst=True)\n",
				"            is_valid = True\n",
				"        except:\n",
				"            is_valid = False\n",
				"    return is_valid\n",
				"\n",
				"\n",
				"def get_s3_bucket_objects(bucket_name, prefix, start_date=None, final_date=None, file_format='csv') -> list:\n",
				"    s3 = boto3.client('s3')\n",
				"    file_list = s3.list_objects_v2(Bucket=bucket_name.removeprefix(\"s3://\").removesuffix('/'),\n",
				"                                    Prefix=prefix.removeprefix(bucket_name))\n",
				"\n",
				"    if 'Contents' in file_list:\n",
				"        content = file_list['Contents']\n",
				"    else:\n",
				"        return []\n",
				"\n",
				"    if 'Key' in content[0]:\n",
				"        if start_date is None:\n",
				"            start_date = '2001-01-01'\n",
				"\n",
				"        if final_date is None:\n",
				"            final_date = '2099-12-31'\n",
				"\n",
				"        start_date = date.fromisoformat(start_date).strftime('%Y%m%d')\n",
				"        final_date = date.fromisoformat(final_date).strftime('%Y%m%d')\n",
				"\n",
				"        obj_list = []\n",
				"        for obj in content:\n",
				"            obj = obj['Key']\n",
				"\n",
				"            if obj.endswith(f\".{file_format}\"):\n",
				"\n",
				"                if start_date <= obj[-12:-4] <= final_date:\n",
				"                    obj_list.append(bucket_name + obj)\n",
				"\n",
				"        obj_list.sort()\n",
				"\n",
				"    else:\n",
				"        return []\n",
				"\n",
				"    return obj_list\n",
				"\n",
				"\n",
				"def s3_bucket_exists(bucket_name) -> bool:\n",
				"    s3 = boto3.client('s3')\n",
				"    obj_list = [obj['Name'] for obj in s3.list_buckets()['Buckets']]\n",
				"    return bucket_name in obj_list\n",
				"\n",
				"\n",
				"def read_source(spark, path, schema):\n",
				"    source_df = (spark.read\n",
				"                    .format(\"csv\")\n",
				"                    .schema(schema)\n",
				"                    .option(\"header\", \"true\")\n",
				"                    .load(path)\n",
				"                    )\n",
				"    return source_df\n",
				"\n",
				"\n",
				"def transform(glueContext, data_frame):\n",
				"    dyf = DynamicFrame.fromDF(data_frame, \n",
				"                                glueContext, \n",
				"                                \"dyf\")\n",
				"\n",
				"    mappings = [('operation', 'string', 'operation', 'char(1)'),\n",
				"                ('policy_id', 'string', 'policy_id', 'bigint'),\n",
				"                ('expiry_date', 'string', 'expiry_date', 'date'),\n",
				"                ('location_name', 'string', 'location_name', 'string'),\n",
				"                ('state_code', 'string', 'state_code', 'string'),\n",
				"                ('region_name', 'string', 'region_name', 'string'),\n",
				"                ('insured_value', 'string', 'insured_value', 'double'),\n",
				"                ('business_type', 'string', 'business_type', 'string'),\n",
				"                ('earthquake', 'string', 'earth_quake', 'char(1)'),\n",
				"                ('flood', 'string', 'flood', 'char(1)')]\n",
				"    dyf = dyf.apply_mapping(mappings)\n",
				"    data_frame = dyf.toDF()\n",
				"\n",
				"    target_df = (data_frame\n",
				"                        .withColumn('file_name',       F.input_file_name())\n",
				"                        .withColumn('year_month_day',  F.expr(\"substring(file_name, length(file_name) -11, 8)\"))\n",
				"                        .withColumn('year',            F.expr(\"substring(year_month_day, 1, 4)\"))\n",
				"                        .withColumn('month',           F.expr(\"substring(year_month_day, 5, 2)\"))\n",
				"                        .withColumn('day',             F.expr(\"substring(year_month_day, 7, 2)\"))\n",
				"                        .drop('operation')\n",
				"                    )\n",
				"    target_df = target_df.orderBy(F.asc('policy_id'), F.desc('year_month_day'))\n",
				"    target_df = target_df.dropDuplicates(['policy_id'])\n",
				"    return target_df\n",
				"\n",
				"\n",
				"def historical_load(target_df, path):\n",
				"    try:\n",
				"        (target_df.write\n",
				"                    .format('delta')\n",
				"                    .mode('overwrite')\n",
				"                    .partitionBy(['year', 'month', 'day'])\n",
				"                    .option(\"overwriteSchema\", \"true\")\n",
				"                    .option(\"path\", path)\n",
				"                    .save()\n",
				"            )\n",
				"    except:\n",
				"        raise ValueError(f\"**** Error saving into the bucket {path}\")\n",
				"\n",
				"\n",
				"def delta_load(spark, delta_df, primary_keys, path):\n",
				"    try:\n",
				"        target_df = DeltaTable.forPath(spark, path)\n",
				"    except:\n",
				"        raise ValueError('**** Target S3 target folder has not found.')\n",
				"\n",
				"    try:\n",
				"        (target_df.alias('target')\n",
				"            .merge(source=delta_df.alias('delta'),\n",
				"                condition=F.expr(dedup_keys_str(primary_keys)))\n",
				"            .whenMatchedUpdateAll()\n",
				"            .whenNotMatchedInsertAll()\n",
				"            ).execute()\n",
				"    except Exception as ex:\n",
				"        raise ValueError(f\"**** Error upserting into bucket {path}. \\\\n {ex}\")\n",
				"\n",
				"\n",
				"def get_schema():\n",
				"    schema_fields = [\n",
				"        ('operation', 'string'),\n",
				"        ('policy_id', 'string'),\n",
				"        ('expiry_date', 'string'),\n",
				"        ('location_name', 'string'),\n",
				"        ('state_code', 'string'),\n",
				"        ('region_name', 'string'),\n",
				"        ('insured_value', 'string'),\n",
				"        ('business_type', 'string'),\n",
				"        ('earthquake', 'string'),\n",
				"        ('flood', 'string')\n",
				"    ]\n",
				"\n",
				"    schema = StructType(\n",
				"                [StructField(field_name, StringType(), True) \n",
				"                for field_name, _ in schema_fields]\n",
				"    )\n",
				"\n",
				"    return schema\n",
				"\n",
				"def main(args, spark, glueContext) -> None:\n",
				"    reprocess_all = (args['reprocess_all'] == 'True')\n",
				"    environment   = args['environment']\n",
				"    file_format   = args['file_format']\n",
				"    s3_bucket     = args['s3_bucket']\n",
				"    start_date    = args['start_date']\n",
				"    final_date    = args['final_date']\n",
				"    days_ago      = 1\n",
				"\n",
				"    ingestion    = 'raw-data'\n",
				"    catalog      = 'glue-catalog'\n",
				"    database     = 'insurance_db'\n",
				"    table_name   = 'policy'\n",
				"    primary_keys = ['policy_id']\n",
				"\n",
				"    prefix_full_load = ingestion + '/' + database + '/' + table_name + '/full-load/'\n",
				"    prefix_cdc_load  = ingestion + '/' + database + '/' + table_name + '/cdc-load/'\n",
				"\n",
				"    source_path_full = s3_bucket + prefix_full_load\n",
				"    source_path_cdc  = s3_bucket + prefix_cdc_load\n",
				"\n",
				"    target_path = s3_bucket + catalog + '/' + database + '/' + table_name + '/'\n",
				"       \n",
				"    if not s3_bucket_exists(s3_bucket.removeprefix(\"s3://\").removesuffix('/')):\n",
				"        raise ValueError('**** Bucket name is invalid.')\n",
				"\n",
				"    if (start_date != 'cron' and not is_valid_date(start_date)) or (\n",
				"            final_date != 'cron' and not is_valid_date(final_date)):\n",
				"        raise ValueError('**** Start or final date is invalid.')\n",
				"\n",
				"    if reprocess_all not in [True, False]:\n",
				"        raise ValueError('**** The parameter reprocess_all must be boolean: (True or False).')\n",
				"\n",
				"    if environment not in ['dev', 'prd']:\n",
				"        raise ValueError('**** The parameter environment must be [dev or prd].')\n",
				"\n",
				"    if reprocess_all:\n",
				"        start_date = '2001-01-01'\n",
				"        final_date = date.today().strftime('%Y-%m-%d')\n",
				"\n",
				"        # TODO: to backup delta table\n",
				"        # to delete delta delta table\n",
				"    else:\n",
				"        if start_date == 'cron':\n",
				"            start_date = (date.today() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
				"        if final_date == 'cron':\n",
				"            final_date = start_date\n",
				"            \n",
				"    if delta_table_exists(spark, target_path):\n",
				"        source_path = source_path_cdc\n",
				"        qtty_before = spark.read.format('delta').load(target_path).count()\n",
				"    else:\n",
				"        source_path = source_path_full\n",
				"        qtty_before = 0\n",
				"        \n",
				"    file_list = get_s3_bucket_objects(bucket_name=s3_bucket,\n",
				"                                        prefix=source_path,\n",
				"                                        start_date=start_date,\n",
				"                                        final_date=final_date,\n",
				"                                        file_format=file_format\n",
				"                                        )\n",
				"\n",
				"    if not file_list:\n",
				"        print('There is not files for loading.')\n",
				"        qtty_src = 0\n",
				"    else:\n",
				"        src_df = read_source(spark,\n",
				"                             file_list, \n",
				"                             get_schema()\n",
				"                            )\n",
				"        qtty_src = src_df.count()\n",
				"        \n",
				"        final_df = transform(glueContext, src_df)\n",
				"\n",
				"        if delta_table_exists(spark, target_path):\n",
				"            print(' >>> Delta loading')\n",
				"            delta_load(spark, \n",
				"                       final_df, \n",
				"                       primary_keys, \n",
				"                       target_path\n",
				"                    )\n",
				"        else:\n",
				"            print(' >>> Historiccal loading')\n",
				"            historical_load(final_df, \n",
				"                            target_path\n",
				"                            )\n",
				"\n",
				"    delta_df = (spark.read\n",
				"                    .format('delta')\n",
				"                    .load(target_path)\n",
				"                )\n",
				"    qtty_after = delta_df.count()\n",
				"\n",
				"    print('Start date  : ', start_date)\n",
				"    print('Final date  : ', final_date)\n",
				"    print('Source path :', source_path)\n",
				"    print('Target path :', target_path)\n",
				"    print('Qtty in DB  :', qtty_before)\n",
				"    print('Qtty in file:', qtty_src)\n",
				"    print('Qtty after  :', qtty_after)\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"\n",
				"args = getResolvedOptions(sys.argv, \n",
				"                              ['JOB_NAME', \n",
				"                              'reprocess_all', \n",
				"                              'file_format', \n",
				"                              'environment', \n",
				"                              's3_bucket',\n",
				"                              'start_date', \n",
				"                              'final_date']\n",
				"                              )\n",
				"\n",
				"t1 = datetime.now()\n",
				"glueContext = GlueContext(SparkContext.getOrCreate())\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)\n",
				"job.init(args[\"JOB_NAME\"], args)\n",
				"\n",
				"try:\n",
				"    main(args, spark, glueContext)\n",
				"except Exception as ex:\n",
				"    print('Error: ', ex)\n",
				"\n",
				"print('Elapsed time: ', datetime.now() - t1)\n",
				"job.commit()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"%stop_session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"pygments_lexer": "python3",
			"version": "3.10.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
