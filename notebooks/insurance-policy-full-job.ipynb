{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"source": [
				"### Historical and CDC loading \n",
				"###### database: Insurance\n",
				"###### table: Policy"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Welcome to the Glue Interactive Sessions Kernel\n",
						"For more information on available magic commands, please type %help in any new cell.\n",
						"\n",
						"Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
						"Current iam_role is arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
						"iam_role has been set to arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole.\n",
						"Previous region: us-east-1\n",
						"Setting new region to: us-east-1\n",
						"Region is set to: us-east-1\n",
						"Current idle_timeout is None minutes.\n",
						"idle_timeout has been set to 5 minutes.\n",
						"Setting Glue version to: 4.0\n",
						"Previous worker type: None\n",
						"Setting new worker type to: G.1X\n",
						"Previous number of workers: None\n",
						"Setting new number of workers to: 2\n"
					]
				}
			],
			"source": [
				"%iam_role arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
				"%region us-east-1\n",
				"%idle_timeout 5\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"The following configurations have been updated: {'conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog', 'datalake-formats': 'delta', 'enable-auto-scaling': 'false', 'JOB_NAME': 'glue-job-policy-insurance-full-load', 's3_bucket': 's3://jamil-datalake-dev/', 'start_date': 'cron', 'final_date': 'cron', 'environment': 'prd', 'file_format': 'csv', 'reprocess_all': 'False'}\n"
					]
				}
			],
			"source": [
				"\n",
				"%%configure -f\n",
				"{\n",
				"    \"conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
				"    \"datalake-formats\":\"delta\",\n",
				"    'enable-auto-scaling': 'false',\n",
				"    'JOB_NAME': 'glue-job-policy-insurance-full-load', \n",
				"    's3_bucket': 's3://jamil-datalake-dev/',\n",
				"    'start_date': 'cron',\n",
				"    'final_date': 'cron',\n",
				"    'environment': 'prd', \n",
				"    'file_format': 'csv',\n",
				"    'reprocess_all': 'False'\n",
				"}\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 1,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Trying to create a Glue session for the kernel.\n",
						"Session Type: etl\n",
						"Worker Type: G.1X\n",
						"Number of Workers: 2\n",
						"Session ID: 52afbd35-dec8-4713-99bc-8de8cae9ee0a\n",
						"Applying the following default arguments:\n",
						"--glue_kernel_version 1.0.4\n",
						"--enable-glue-datacatalog true\n",
						"--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
						"--datalake-formats delta\n",
						"--enable-auto-scaling false\n",
						"--JOB_NAME glue-job-policy-insurance-full-load\n",
						"--s3_bucket s3://jamil-datalake-dev/\n",
						"--start_date cron\n",
						"--final_date cron\n",
						"--environment prd\n",
						"--file_format csv\n",
						"--reprocess_all False\n",
						"Waiting for session 52afbd35-dec8-4713-99bc-8de8cae9ee0a to get into ready status...\n",
						"Session 52afbd35-dec8-4713-99bc-8de8cae9ee0a has been created.\n",
						"\n"
					]
				}
			],
			"source": [
				"import sys\n",
				"import boto3\n",
				"import pyspark.sql.functions as F\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql import DataFrame\n",
				"from pyspark.sql.types import StructType, StructField, StringType\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from delta import DeltaTable\n",
				"from datetime import date, datetime, timedelta\n",
				"from dateutil.parser import parse\n",
				"from re import sub\n",
				"\n",
				"t1 = datetime.now()\n",
				"glueContext = GlueContext(SparkContext.getOrCreate())\n",
				"spark = glueContext.spark_session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# convert a text to a snake case string\n",
				"def to_snake_case(text):\n",
				"    return '_'.join(\n",
				"                    sub('([A-Z][a-z]+)', r' \\1',\n",
				"                    sub('([A-Z]+)', r' \\1',\n",
				"                        text.replace('-', ' ')\n",
				"                        )).split()\n",
				"                    ).lower()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def dedup_keys_str(primary_keys: list) -> str:\n",
				"\n",
				"    condition_list = [f'target.{key} = delta.{key}' for key in primary_keys]\n",
				"\n",
				"    dedup_str = ' AND '.join(condition_list)\n",
				"            \n",
				"    return dedup_str\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def table_exists(database, table) -> bool:\n",
				"    \n",
				"    exist = spark.sql(f\"\"\" select * \n",
				"                            from {database}.{table} \n",
				"                            limit 1\n",
				"                        \"\"\")\n",
				"            \n",
				"    return bool(exist.count() > 0) \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def delta_table_exists(path) -> bool:\n",
				"\n",
				"    exist = False\n",
				"    try:\n",
				"        delta = DeltaTable.forPath(spark, path)\n",
				"        exist = True\n",
				"    except:\n",
				"        exist = False\n",
				"\n",
				"    return exist    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"\n",
				"def is_valid_date(date_ymd) -> bool:\n",
				"\n",
				"    is_valid = False\n",
				"\n",
				"    if date_ymd:\n",
				"        try:\n",
				"            parse(timestr=date_ymd, yearfirst=True, dayfirst=True)\n",
				"            is_valid = True\n",
				"        except:\n",
				"            is_valid = False\n",
				"    \n",
				"    return is_valid\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def get_s3_bucket_objects(bucket_name, prefix, start_date=None, final_date=None, file_format='csv') -> list:\n",
				"      \n",
				"    s3 = boto3.client('s3')\n",
				"    file_list = s3.list_objects_v2(Bucket= bucket_name.removeprefix(\"s3://\").removesuffix('/'), \n",
				"                                   Prefix= prefix.removeprefix(bucket_name))\n",
				"\n",
				"    if 'Contents' in file_list:\n",
				"        content = file_list['Contents']\n",
				"    else:\n",
				"        return []\n",
				"\n",
				"\n",
				"    if 'Key' in content[0]:\n",
				"\n",
				"        if start_date == None:\n",
				"            start_date = '2001-01-01' ## all possible dates\n",
				"\n",
				"        if final_date == None:\n",
				"            final_date = '2099-12-31'\n",
				"\n",
				"        start_date = date.fromisoformat(start_date).strftime('%Y%m%d')\n",
				"        final_date = date.fromisoformat(final_date).strftime('%Y%m%d')\n",
				"        \n",
				"        obj_list = []\n",
				"        for obj in content:\n",
				"            obj = obj['Key']\n",
				"            \n",
				"            if obj.endswith(f\".{file_format}\"):\n",
				"                \n",
				"                if obj[-12:-4] >= start_date and obj[-12:-4] <= final_date: \n",
				"                    obj_list.append(bucket_name + obj)\n",
				"    \n",
				"        obj_list.sort()\n",
				"\n",
				"    else:\n",
				"        return []   \n",
				"       \n",
				"    return obj_list\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def s3_bucket_exists(bucket_name) -> bool:\n",
				"    \n",
				"    s3 = boto3.client('s3')\n",
				"    obj_list = [obj['Name'] for obj in s3.list_buckets()['Buckets'] ]\n",
				"\n",
				"    return bucket_name in obj_list\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# reading source data file\n",
				"\n",
				"def read_source(path, schema):\n",
				"            \n",
				"    source_df = (spark.read\n",
				"                    .format(\"csv\")\n",
				"                    .schema(schema)\n",
				"                    .option(\"header\", \"true\")\n",
				"                    .load(path)\n",
				"                )\n",
				"\n",
				"    return source_df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# transforming data and columns\n",
				"\n",
				"def transform(data_frame):\n",
				"      \n",
				"      # apply mapping\n",
				"      dyf = DynamicFrame.fromDF(data_frame, glueContext, \"dyf\")\n",
				"\n",
				"      mappings = [('operation', 'string', 'operation', 'char(1)'), \n",
				"                  ('policy_id', 'string', 'policy_id', 'bigint'), \n",
				"                  ('expiry_date', 'string', 'expiry_date', 'date'), \n",
				"                  ('location_name', 'string', 'location_name', 'string'), \n",
				"                  ('state_code', 'string', 'state_code', 'string'), \n",
				"                  ('region_name', 'string', 'region_name', 'string'), \n",
				"                  ('insured_value', 'string', 'insured_value', 'double'), \n",
				"                  ('business_type', 'string', 'business_type', 'string'), \n",
				"                  ('earthquake', 'string', 'earth_quake', 'char(1)'), \n",
				"                  ('flood', 'string', 'flood', 'char(1)')]\n",
				"#                  ('update_date', 'string', 'update_date', 'date')]\n",
				"\n",
				"      dyf = dyf.apply_mapping(mappings)\n",
				"      data_frame = dyf.toDF()\n",
				"\n",
				"      target_df = (data_frame\n",
				"                        .withColumn('file_name',      F.input_file_name())\n",
				"                        .withColumn('year_month_day', F.expr(\"substring(file_name, length(file_name) -11, 8)\"))\n",
				"                        .withColumn('year',           F.expr(\"substring(year_month_day, 1, 4)\"))\n",
				"                        .withColumn('month',          F.expr(\"substring(year_month_day, 5, 2)\"))\n",
				"                        .withColumn('day',            F.expr(\"substring(year_month_day, 7, 2)\"))\n",
				"                        .drop('operation')\n",
				"                        .dropDuplicates()\n",
				"                  )\n",
				"\n",
				"      #target_df = target_df.select([F.col(c) for c in target_df.columns])\n",
				"      return target_df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 11,
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# writing first time\n",
				"\n",
				"def historical_load(target_df, path):\n",
				"\n",
				"    try:\n",
				"        (target_df.write\n",
				"            .format('delta')\n",
				"            .mode('overwrite') \n",
				"            .partitionBy(['year', 'month', 'day'])\n",
				"            .option(\"overwriteSchema\", \"true\")\n",
				"            .option(\"path\", path)\n",
				"            .save()\n",
				"        )\n",
				"    except:\n",
				"        raise ValueError(f\"**** Error saving into the bucket {path}\")\n",
				"        "
			]
		},
		{
			"cell_type": "code",
			"execution_count": 12,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"# upsert\n",
				"\n",
				"def delta_load(delta_df, primary_keys, path):\n",
				"\n",
				"    try:\n",
				"        target_df = DeltaTable.forPath(spark, path)\n",
				"    except:\n",
				"        raise ValueError('**** Target S3 target folder has not found.')\n",
				"\n",
				"    try:\n",
				"        (target_df.alias('target')\n",
				"                .merge( source    = delta_df.alias('delta'),\n",
				"                        condition = F.expr(dedup_keys_str(primary_keys)))\n",
				"                .whenMatchedUpdateAll()\n",
				"                .whenNotMatchedInsertAll()\n",
				"        ).execute()\n",
				"    except:\n",
				"        raise ValueError(f\"**** Error upserting into bucket {path}\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 13,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"def get_schema():\n",
				"    schema_fields = [\n",
				"            ('operation', 'string'),\n",
				"            ('policy_id', 'string'),\n",
				"            ('expiry_date', 'string'),\n",
				"            ('location_name', 'string'),\n",
				"            ('state_code', 'string'),\n",
				"            ('region_name', 'string'),\n",
				"            ('insured_value', 'string'),\n",
				"            ('business_type', 'string'),\n",
				"            ('earthquake', 'string'),\n",
				"            ('flood', 'string')\n",
				"        ]\n",
				"\n",
				"    schema = StructType(\n",
				"                [StructField(field_name, StringType(), True) for field_name, _ in schema_fields]\n",
				"            )\n",
				"    \n",
				"    return schema"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"\n",
				"def main(args) -> None:\n",
				"    reprocess_all= (args['reprocess_all'] == 'True') # when reprocess_all = true, all data will be deleted and reloaded\n",
				"    environment  = args['environment']\n",
				"    file_format  = args['file_format'] # not nullable\n",
				"    s3_bucket    = args['s3_bucket']   # not nullable\n",
				"    start_date   = args['start_date']  # not nullable\n",
				"    final_date   = args['final_date']  # not nullable\n",
				"    days_ago     = 0\n",
				"\n",
				"    ingestion    = 'raw-data'\n",
				"    catalog      = 'glue-catalog'\n",
				"    database     = 'insurance_db'\n",
				"    table_name   = 'policy'\n",
				"    primary_keys = ['policy_id']\n",
				"\n",
				"    prefix_full_load = ingestion +'/'+ database +'/'+ table_name + '/full-load/'\n",
				"    prefix_cdc_load  = ingestion +'/'+ database +'/'+ table_name + '/cdc-load/'\n",
				"\n",
				"    source_path_full = s3_bucket + prefix_full_load \n",
				"    source_path_cdc = s3_bucket + prefix_cdc_load\n",
				"\n",
				"    target_path      = s3_bucket + catalog +'/'+ database +'/'+ table_name + '/'\n",
				"\n",
				"    if not s3_bucket_exists(s3_bucket.removeprefix(\"s3://\").removesuffix('/')):\n",
				"        raise ValueError('**** Bucket name is invalid.')\n",
				"    \n",
				"    if (start_date != 'cron' and not is_valid_date(start_date)) or (final_date != 'cron' and not is_valid_date(final_date)):\n",
				"        raise ValueError('**** Start or final date is invalid.')\n",
				"    \n",
				"    if reprocess_all not in [True, False]:\n",
				"        raise ValueError('**** The parameter reprocess_all must be boolean: (True or False).')       \n",
				"        \n",
				"    if environment not in ['dev', 'prd']:\n",
				"        raise ValueError('**** The parameter environment must be [dev or prd].')\n",
				"\n",
				"    if reprocess_all: \n",
				"        start_date  = '2001-01-01'\n",
				"        final_date  = date.today().strftime('%Y-%m-%d')\n",
				"        \n",
				"        ## TODO: \n",
				"         # to backup delta table \n",
				"         # to delete delta delta table\n",
				"    else:\n",
				"        if start_date == 'cron':\n",
				"            start_date = (date.today() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
				"        if final_date == 'cron':\n",
				"            final_date = start_date\n",
				"\n",
				"\n",
				"    if delta_table_exists(target_path):\n",
				"        source_path = source_path_cdc\n",
				"        qtty_before = spark.read.format('delta').load(target_path).count()\n",
				"    else: \n",
				"        source_path = source_path_full\n",
				"        qtty_before = 0\n",
				"            \n",
				"\n",
				"    #get file names from source path (cdc or full)\n",
				"    file_list = get_s3_bucket_objects(bucket_name=  s3_bucket, \n",
				"                                        prefix=     source_path,\n",
				"                                        start_date= start_date,\n",
				"                                        final_date= final_date,\n",
				"                                        file_format=file_format)\n",
				"\n",
				"    if not file_list:\n",
				"        print('No one file for loading.')\n",
				"        qtty_src   = 0\n",
				"    else:\n",
				"        # reading files between start and final dates\n",
				"        src_df = read_source(file_list, get_schema())\n",
				"        qtty_src = src_df.count()\n",
				"        \n",
				"        # transforming\n",
				"        final_df = transform(src_df)\n",
				"\n",
				"        # loading historical or delta data\n",
				"        if delta_table_exists(target_path):\n",
				"            print(' >>> Delta loading')\n",
				"            delta_load(final_df, primary_keys, target_path)\n",
				"        else:\n",
				"            print(' >>> Historiccal loading')\n",
				"            historical_load(final_df, target_path)\n",
				"        \n",
				"    #checking data loaded\n",
				"    delta_df = (spark.read\n",
				"                    .format('delta')\n",
				"                    .load(target_path)\n",
				"                )\n",
				"    qtty_after = delta_df.count()\n",
				"\n",
				"    print('Start date  : ', start_date)\n",
				"    print('Final date  : ', final_date)\n",
				"    print('Source path :', source_path)\n",
				"    print('Target path :', target_path)\n",
				"    print('Qtty in DB  :', qtty_before)\n",
				"    print('Qtty in file:', qtty_src)\n",
				"    print('Qtty after  :', qtty_after)\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						" >>> Delta loading\n",
						"Start date  :  2024-01-29\n",
						"Final date  :  2024-01-29\n",
						"Source path : s3://jamil-datalake-dev/raw-data/insurance_db/policy/cdc-load/\n",
						"Target path : s3://jamil-datalake-dev/glue-catalog/insurance_db/policy/\n",
						"Qtty in DB  : 32\n",
						"Qtty in file: 6\n",
						"Qtty after  : 33\n",
						"Elapsed time:  0:01:31.850433\n",
						"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
					]
				}
			],
			"source": [
				"\n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME', 'reprocess_all', 'file_format', 'environment', 's3_bucket', 'start_date', 'final_date'])\n",
				"\n",
				"job = Job(glueContext)\n",
				"job.init(args[\"JOB_NAME\"], args)\n",
				"\n",
				"try:\n",
				"    main(args)\n",
				"except Exception as ex:\n",
				"    print('Error: ', ex)    \n",
				"\n",
				"print('Elapsed time: ', datetime.now() - t1)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\"\\nsrc_df = read_source(['s3://jamil-datalake-dev/raw-data/insurance_db/policy/cdc-load/cdc-load-20240124.csv'], '2024-01-24','2024-01-24')\\nfinal_df = transform(src_df)\\nsrc_df.show()\\nfinal_df.show()\\n\"\n"
					]
				}
			],
			"source": [
				"job.commit()\n",
				"\n",
				"'''\n",
				"src_df = read_source(['s3://jamil-datalake-dev/raw-data/insurance_db/policy/cdc-load/cdc-load-20240124.csv'], '2024-01-24','2024-01-24')\n",
				"final_df = transform(src_df)\n",
				"src_df.show()\n",
				"final_df.show()\n",
				"'''"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Stopping session: 52afbd35-dec8-4713-99bc-8de8cae9ee0a\n",
						"Stopped session.\n"
					]
				}
			],
			"source": [
				"%stop_session"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": []
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"pygments_lexer": "python3",
			"version": "3.10.10"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
