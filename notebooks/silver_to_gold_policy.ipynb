{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iam_role is arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
      "iam_role has been set to arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole.\n",
      "Previous region: us-east-1\n",
      "Setting new region to: us-east-1\n",
      "Region is set to: us-east-1\n",
      "Current idle_timeout is 5 minutes.\n",
      "idle_timeout has been set to 5 minutes.\n",
      "Setting Glue version to: 4.0\n",
      "Previous worker type: G.1X\n",
      "Setting new worker type to: G.1X\n",
      "Previous number of workers: 2\n",
      "Setting new number of workers to: 2\n"
     ]
    }
   ],
   "source": [
    "%iam_role arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
    "%region us-east-1\n",
    "%idle_timeout 5\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following configurations have been updated: {'conf': 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog', 'datalake-formats': 'delta', 'enable-auto-scaling': 'false', 'JOB_NAME': 'glue-test-job', 'start_date': '2024-01-24'}\n"
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    \"datalake-formats\":\"delta\",\n",
    "    #\"additional-python-modules\": \"awswrangler\",\n",
    "    'enable-auto-scaling': 'false',\n",
    "    'JOB_NAME': 'glue-test-job',\n",
    "    'start_date': '2024-01-24'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create a Glue session for the kernel.\n",
      "Session Type: etl\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 2\n",
      "Session ID: aa4ec36e-e95b-450a-98d1-9c5a970fcdb0\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 1.0.4\n",
      "--enable-glue-datacatalog true\n",
      "--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
      "--datalake-formats delta\n",
      "--enable-auto-scaling false\n",
      "--JOB_NAME glue-test-job\n",
      "--start_date 2024-01-24\n",
      "Waiting for session aa4ec36e-e95b-450a-98d1-9c5a970fcdb0 to get into ready status...\n",
      "Session aa4ec36e-e95b-450a-98d1-9c5a970fcdb0 has been created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as sqlFunc\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from datetime import datetime, date, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply groupBy and aggregate functions in a DyF\n",
    "def sparkAggregate(glueContext, parentFrame, groups, aggs, transformation_ctx) -> DynamicFrame:\n",
    "\n",
    "    aggsFuncs = []\n",
    "    aggsColNames = []\n",
    "    for column, func in aggs:\n",
    "        aggsFuncs.append(getattr(sqlFunc, func)(column))\n",
    "        aggsColNames.append(func +'_'+ column)\n",
    "   \n",
    "    result_df = (\n",
    "                parentFrame.toDF().groupBy(*groups).agg(*aggsFuncs)\n",
    "                if len(groups) > 0\n",
    "                else parentFrame.toDF().agg(*aggsFuncs)\n",
    "            )\n",
    "    cols = result_df.columns\n",
    "    \n",
    "    for c in range(len(aggsColNames)):\n",
    "        result_df = result_df.withColumnRenamed(cols[c], aggsColNames[c])\n",
    "\n",
    "    return DynamicFrame.fromDF(result_df, glueContext, transformation_ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"start_date\"])\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_date  = args['start_date']\n",
    "days_ago    = 1\n",
    "bucket_name = 'jamil-datalake-dev'\n",
    "catalog     = \"glue-catalog\"\n",
    "source_db   = 'insurance_db'\n",
    "target_db   = \"insurance_prd\"\n",
    "source_table= \"policy\"\n",
    "target_table= \"policy_prod\"\n",
    "primary_keys= [\"policy_id\"]\n",
    "partitions  = [\"year\", \"month\", \"day\"]\n",
    "target_path = f\"s3://{bucket_name}/{catalog}/{target_db}/{target_table}/\"\n",
    "\n",
    "if start_date == 'cron' or not start_date:\n",
    "    start_date = (date.today() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "\n",
    "part_year = start_date[:4]\n",
    "part_month= start_date[5:7]\n",
    "part_day  = start_date[8:]\n",
    "\n",
    "target_exists = target_table in [tb.name for tb in spark.catalog.listTables(dbName=target_db)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "source_df = glueContext.create_data_frame.from_catalog( database   = source_db, \n",
    "                                                        table_name = source_table\n",
    "    )\n",
    "\n",
    "source_dyf = DynamicFrame.fromDF(source_df,\n",
    "                                 glueContext,\n",
    "                                 \"source_dyf\"\n",
    "    )\n",
    "\n",
    "if target_exists:\n",
    "    src_dyf = source_dyf.filter(\n",
    "                    f= lambda x: x[partitions[0]] == start_date[:4]\n",
    "                             and x[partitions[1]] == start_date[5:7]\n",
    "                             and x[partitions[2]] == start_date[8:]\n",
    "                )\n",
    "else:\n",
    "    ymd_dyf = sparkAggregate(\n",
    "                        glueContext = glueContext,\n",
    "                        parentFrame = source_dyf,\n",
    "                        groups      = ['year','month','day'],\n",
    "                        aggs        = [['year_month_day', 'first']],\n",
    "                        transformation_ctx='ymd_dyf'\n",
    "    )\n",
    "    \n",
    "    src_dyf = source_dyf.join(frame2=ymd_dyf, \n",
    "                              paths1=['year','month','day'], \n",
    "                              paths2=['year','month','day']\n",
    "                              )    \n",
    "\n",
    "# TODO:\n",
    "## src_dyf deduplication\n",
    "\n",
    "src_dyf = ApplyMapping.apply(\n",
    "                frame=src_dyf,\n",
    "                mappings=[\n",
    "                    (\"policy_id\", \"long\", \"policy_id\", \"long\"),\n",
    "                    (\"expiry_date\", \"date\", \"expiry_date\", \"date\"),\n",
    "                    (\"location_name\", \"string\", \"location_name\", \"string\"),\n",
    "                    (\"state_code\", \"string\", \"state_code\", \"string\"),\n",
    "                    (\"region_name\", \"string\", \"region_name\", \"string\"),\n",
    "                    (\"insured_value\", \"double\", \"insured_value\", \"double\"),\n",
    "                    (\"business_type\", \"string\", \"business_type\", \"string\"),\n",
    "                    (\"flood\", \"string\", \"flood\", \"string\"),\n",
    "                    (\"file_name\", \"string\", \"file_name\", \"string\"),\n",
    "                    (\"year_month_day\", \"string\", \"year_month_day\", \"string\"),\n",
    "                    (\"year\", \"string\", \"year\", \"string\"),\n",
    "                    (\"month\", \"string\", \"month\", \"string\"),\n",
    "                    (\"day\", \"string\", \"day\", \"string\"),\n",
    "                ]\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## verificar se a tabela existe\n",
    "## se não existir, não executar a leitura full da target\n",
    "\n",
    "src_df = src_dyf.toDF()\n",
    "\n",
    "if not target_exists:\n",
    "    target_df = src_df\n",
    "else:\n",
    "    target_df = (glueContext.create_dynamic_frame\n",
    "                             .from_catalog(database  = target_db, \n",
    "                                           table_name= target_table\n",
    "                                           )\n",
    "                ).toDF()\n",
    "   \n",
    "\n",
    "    # filter for deletions\n",
    "    not_match_df = (target_df.alias('target')\n",
    "                             .join(src_df.alias('src'), \n",
    "                                   #'policy_id',\n",
    "                                   sqlFunc.col(\"target.policy_id\") == sqlFunc.col(\"src.policy_id\"),\n",
    "                                   'left')\n",
    "                          .where('src.policy_id is null or target.expiry_date > src.expiry_date' )\n",
    "                          .select('target.*')\n",
    "                )\n",
    "    \n",
    "    target_df = not_match_df.union(src_df)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_dyf = DynamicFrame.fromDF(target_df, glueContext, 'target_dyf')\n",
    "\n",
    "# delete existing data\n",
    "if target_dyf.toDF().count() > 0:\n",
    "    glueContext.purge_table(target_db, \n",
    "                            target_table, \n",
    "                            {\"retentionPeriod\": 0, \n",
    "                             \"manifestFilePath\": target_path + \"manifest/\"},\n",
    "                            'target_dyf'\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "additional_options={\n",
    "        \"enableUpdateCatalog\": True,\n",
    "        \"updateBehavior\": \"UPDATE_IN_DATABASE\"        \n",
    "    }\n",
    "\n",
    "s3sink = glueContext.getSink(\n",
    "    connection_type     = \"s3\",\n",
    "    path                = target_path,\n",
    "    partitionKeys       = partitions,\n",
    "    compression         = \"snappy\",\n",
    "    enableUpdateCatalog = True,\n",
    "    updateBehavior      = \"UPDATE_IN_DATABASE\"\n",
    ")\n",
    "    \n",
    "s3sink.setCatalogInfo(\n",
    "    catalogDatabase  = target_db, \n",
    "    catalogTableName = target_table\n",
    ")\n",
    "    \n",
    "s3sink.setFormat(\"glueparquet\", useGlueParquetWriter=True)\n",
    "\n",
    "final_dyf = s3sink.writeFrame(target_dyf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping session: aa4ec36e-e95b-450a-98d1-9c5a970fcdb0\n",
      "Stopped session.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
