{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%iam_role arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
    "%region us-east-1\n",
    "%idle_timeout 5\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    'JOB_NAME': 'silter_to_gold_policy',\n",
    "    \"datalake-formats\":\"delta\",\n",
    "    'enable-auto-scaling': 'false',\n",
    "    'start_date': '20240203'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as sqlFunc\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue import DynamicFrame\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from datetime import date, timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_source_data(glue_context, source_db, source_table):\n",
    "    source_df = (glue_context\n",
    "                        .create_data_frame\n",
    "                        .from_catalog(database   = source_db, \n",
    "                                      table_name = source_table)\n",
    "                        )\n",
    "    \n",
    "    return DynamicFrame.fromDF(source_df, \n",
    "                                glue_context, \n",
    "                                \"source_dyf\")\n",
    "\n",
    "def spark_aggregate(glue_context, parent_frame, groups, aggs, transformation_ctx):\n",
    "    aggs_funcs          = [getattr(sqlFunc, func)(column) for column, func in aggs]\n",
    "    aggs_col_func_names = [func + '_' + column for column, func in aggs]\n",
    "\n",
    "    result_df = (\n",
    "            parent_frame.toDF()\n",
    "                        .groupBy(*groups)\n",
    "                        .agg(*aggs_funcs)\n",
    "            if len(groups) > 0\n",
    "            else parent_frame.toDF()\n",
    "                            .agg(*aggs_funcs)\n",
    "    )\n",
    "\n",
    "    for c in range(len(aggs_col_func_names)):\n",
    "        result_df = result_df.withColumnRenamed(result_df.columns[c + len(groups)], aggs_col_func_names[c])\n",
    "    \n",
    "    return DynamicFrame.fromDF(result_df, \n",
    "                                glue_context, \n",
    "                                transformation_ctx)\n",
    "\n",
    "def apply_mappings(frame):\n",
    "    return ApplyMapping.apply(\n",
    "                frame=frame,\n",
    "                mappings=[\n",
    "                    (\"policy_id\",       \"long\",   \"policy_id\",      \"long\"),\n",
    "                    (\"expiry_date\",     \"date\",   \"expiry_date\",    \"date\"),\n",
    "                    (\"location_name\",   \"string\", \"location_name\",  \"string\"),\n",
    "                    (\"state_code\",      \"string\", \"state_code\",     \"string\"),\n",
    "                    (\"region_name\",     \"string\", \"region_name\",    \"string\"),\n",
    "                    (\"insured_value\",   \"double\", \"insured_value\",  \"double\"),\n",
    "                    (\"business_type\",   \"string\", \"business_type\",  \"string\"),\n",
    "                    (\"flood\",           \"string\", \"flood\",          \"string\"),\n",
    "                    (\"file_name\",       \"string\", \"file_name\",      \"string\"),\n",
    "                    (\"year_month_day\",  \"string\", \"year_month_day\", \"string\"),\n",
    "                    (\"year\",            \"string\", \"year\",           \"string\"),\n",
    "                    (\"month\",           \"string\", \"month\",          \"string\"),\n",
    "                    (\"day\",             \"string\", \"day\",            \"string\"),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "def write_to_table(glue_context, target_dyf, target_db, target_table, target_path, partitions):\n",
    "    try:\n",
    "        s3sink = glue_context.getSink(\n",
    "                    connection_type=\"s3\",\n",
    "                    path=target_path,\n",
    "                    partitionKeys=partitions,\n",
    "                    compression=\"snappy\",\n",
    "                    enableUpdateCatalog=True,\n",
    "                    updateBehavior=\"UPDATE_IN_DATABASE\"\n",
    "        )\n",
    "\n",
    "        s3sink.setCatalogInfo(\n",
    "                    catalogDatabase =target_db,\n",
    "                    catalogTableName=target_table\n",
    "        )\n",
    "\n",
    "        s3sink.setFormat(\"glueparquet\", useGlueParquetWriter=True)\n",
    "\n",
    "        s3sink.writeFrame(target_dyf)\n",
    "    except Exception as ex:\n",
    "        ValueError (f'**** Error saving into database. \\\\n{ex}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_etl(glue_context, spark, start_date, days_ago, bucket_name, catalog, source_db, target_db, source_table, target_table, partitions):\n",
    "    \n",
    "    target_path = f\"s3://{bucket_name}/{catalog}/{target_db}/{target_table}/\"\n",
    "    \n",
    "    if start_date == 'cron' or not start_date:\n",
    "        start_date = (date.today() - timedelta(days=days_ago)).strftime('%Y%m%d')\n",
    "    \n",
    "    part_year  = start_date[:4]\n",
    "    part_month = start_date[4:6]\n",
    "    part_day   = start_date[6:]\n",
    "    \n",
    "    print(part_day, part_month, part_year)\n",
    "    \n",
    "    target_exists = target_table in [tb.name for tb in spark.catalog.listTables(dbName=target_db)]\n",
    "\n",
    "    source_dyf = read_source_data(glue_context, source_db, source_table)\n",
    "\n",
    "    if target_exists:\n",
    "        src_dyf = source_dyf.filter(\n",
    "                                f=lambda x:     x[partitions[0]] == part_year\n",
    "                                            and x[partitions[1]] == part_month\n",
    "                                            and x[partitions[2]] == part_day\n",
    "                            )\n",
    "    else:\n",
    "        ymd_dyf = spark_aggregate(\n",
    "                            glue_context = glue_context,\n",
    "                            parent_frame = source_dyf,\n",
    "                            groups       = [],\n",
    "                            aggs         = [['year_month_day', 'first']],\n",
    "                            transformation_ctx='ymd_dyf'\n",
    "        )\n",
    "        \n",
    "        print(ymd_dyf.toDF().show())\n",
    "        \n",
    "        src_dyf = source_dyf.join(\n",
    "                                frame2=ymd_dyf,\n",
    "                                paths1=['year_month_day'],\n",
    "                                paths2=['first_year_month_day']\n",
    "        )\n",
    "\n",
    "    src_dyf = apply_mappings(src_dyf)\n",
    "\n",
    "    src_df = src_dyf.toDF()\n",
    "    qtty_src = src_df.count()\n",
    "\n",
    "    if not target_exists:\n",
    "        target_df = src_dyf.toDF()\n",
    "        qtty_before = target_df.count()\n",
    "    else:\n",
    "        target_df = (glue_context\n",
    "                            .create_dynamic_frame\n",
    "                            .from_catalog(database   = target_db,\n",
    "                                          table_name = target_table\n",
    "                            )\n",
    "                    ).toDF()\n",
    "        qtty_before = target_df.count()\n",
    "\n",
    "        # filter for deletions\n",
    "        not_match_df = (target_df.alias('target')\n",
    "                                .join(  src_df.alias('src'),\n",
    "                                        sqlFunc.col(\"target.policy_id\") == sqlFunc.col(\"src.policy_id\"),\n",
    "                                        'left')\n",
    "                                .where('src.policy_id is null') #or target.expiry_date > src.expiry_date')\n",
    "                                .select('target.*')\n",
    "        )\n",
    "\n",
    "        target_df = not_match_df.union(src_df)\n",
    "        \n",
    "    target_dyf = DynamicFrame.fromDF(target_df, glue_context, 'target_dyf')\n",
    "    qtty_after = target_df.count()\n",
    "\n",
    "    \n",
    "    if target_exists:\n",
    "        glue_context.purge_table(\n",
    "                                target_db,\n",
    "                                target_table,\n",
    "                                {\"retentionPeriod\": 0,\n",
    "                                \"manifestFilePath\": target_path + \"manifest/\"},\n",
    "                                \"target_dyf\"\n",
    "        )\n",
    "    \n",
    "    write_to_table(glue_context,\n",
    "                   target_dyf, \n",
    "                   target_db, \n",
    "                   target_table, \n",
    "                   target_path, \n",
    "                   partitions\n",
    "    )\n",
    "\n",
    "    print('Start date  :', start_date)\n",
    "    print('Source db   :', source_db)\n",
    "    print('Qtty before :', qtty_before)\n",
    "    print('Qtty source :', qtty_src)\n",
    "    print('Qtty after  :', qtty_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"start_date\"])\n",
    "t1 = datetime.now()\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "        run_etl(glue_context=glueContext,\n",
    "                spark=spark,\n",
    "                start_date=args['start_date'], \n",
    "                days_ago=1, \n",
    "                bucket_name='jamil-datalake-dev', \n",
    "                catalog=\"glue-catalog\",\n",
    "                source_db='insurance_db', \n",
    "                target_db=\"insurance_prd\", \n",
    "                source_table=\"policy\", \n",
    "                target_table=\"policy_prod\",\n",
    "                partitions=[\"year\", \"month\", \"day\"]\n",
    "        )\n",
    "except Exception as ex:\n",
    "        print('Erro: ', ex)\n",
    "        \n",
    "print('Elapsed time: ', datetime.now() - t1)\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
