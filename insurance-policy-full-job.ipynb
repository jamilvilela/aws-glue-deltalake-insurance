{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			},
			"source": [
				"### Historical and CDC loading \n",
				"###### database: Insurance\n",
				"###### table: Policy"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"%iam_role arn:aws:iam::331504768406:role/service-role/AWSGlueServiceRole\n",
				"%region us-east-1\n",
				"%idle_timeout 5\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"\n",
				"%%configure -f\n",
				"{\n",
				"    \"conf\": \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
				"    \"datalake-formats\":\"delta\",\n",
				"    'enable-auto-scaling': 'true',\n",
				"    'JOB_NAME': 'glue-job-policy-insurance-full-load', \n",
				"    's3_bucket': 's3://jamil-datalake-dev/',\n",
				"    'start_date': '2020-01-01',\n",
				"    'final_date': '2024-12-31',\n",
				"    'environment': 'prd' ## ['prd', 'dev']\n",
				"}\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"import sys\n",
				"import pyspark.sql.functions as F\n",
				"from pyspark.context import SparkContext\n",
				"from pyspark.sql import DataFrame\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue.dynamicframe import DynamicFrame\n",
				"from delta import DeltaTable\n",
				"from datetime import datetime, timedelta\n",
				"from dateutil.parser import parse\n",
				"from re import sub\n",
				"\n",
				"glueContext = GlueContext(SparkContext.getOrCreate())\n",
				"spark = glueContext.spark_session\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"# convert a text to a snake case string\n",
				"def to_snake_case(text):\n",
				"    return '_'.join(\n",
				"                    sub('([A-Z][a-z]+)', r' \\1',\n",
				"                    sub('([A-Z]+)', r' \\1',\n",
				"                        text.replace('-', ' ')\n",
				"                        )).split()\n",
				"                    ).lower()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"def dedup_keys_str(primary_keys: list) -> str:\n",
				"\n",
				"    dedup_str = ''\n",
				"    condition_list = []\n",
				"\n",
				"    for index in range(0, len(primary_keys) ):\n",
				"        condition_list.append(f'target.{primary_keys[index]} = delta.{primary_keys[index]}')\n",
				"\n",
				"    if len(condition_list) > 1:\n",
				"        dedup_str = ' AND '.join(condition_list)\n",
				"    elif len(condition_list) == 1:\n",
				"        dedup_str = condition_list[0]\n",
				"\n",
				"    return dedup_str\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"def table_exists(database, table) -> bool:\n",
				"    \n",
				"    exist = spark.sql(f\"\"\" select * \n",
				"                            from {database}.{table} \n",
				"                            limit 1\n",
				"                        \"\"\")\n",
				"    \n",
				"    print('table exists:', exist.count())\n",
				"        \n",
				"    return bool(exist.count() > 0) \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"def delta_table_exists(path) -> bool:\n",
				"\n",
				"    exist = False\n",
				"    try:\n",
				"        delta = DeltaTable.forPath(spark, path)\n",
				"        exist = True\n",
				"    except:\n",
				"        exist = False\n",
				"\n",
				"    return exist    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"\n",
				"def is_valid_date(date_ymd) -> bool:\n",
				"\n",
				"    is_valid = False\n",
				"\n",
				"    if date_ymd:\n",
				"        try:\n",
				"            parse(timestr=date_ymd, yearfirst=True, dayfirst=True)\n",
				"            is_valid = True\n",
				"        except:\n",
				"            is_valid = False\n",
				"    \n",
				"    return is_valid\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"def s3_bucket_exists(s3_bucket) -> bool:\n",
				"    return True\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"# reading source data file\n",
				"\n",
				"def read_source(path, start_dt, final_dt) -> DataFrame:\n",
				"        \n",
				"    source_df = (spark.read\n",
				"                    .format('csv')\n",
				"                    .option('header', True)\n",
				"                    .load(path + '*.csv')\n",
				"                    .filter(f\" expiry_date >= '{start_dt}' and expiry_date <= '{final_dt}' \")\n",
				"                )\n",
				"\n",
				"    return source_df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"# transforming\n",
				"def transform(data_frame) -> DataFrame:\n",
				"      \n",
				"      # apply mapping\n",
				"      dyf = DynamicFrame.fromDF(data_frame, glueContext, \"dyf\")\n",
				"\n",
				"      mappings = [('operation', 'string', 'operation', 'char(1)'), \n",
				"                  ('policy_id', 'string', 'policy_id', 'bigint'), \n",
				"                  ('expiry_date', 'string', 'expiry_date', 'date'), \n",
				"                  ('location_name', 'string', 'location_name', 'string'), \n",
				"                  ('state_code', 'string', 'state_code', 'string'), \n",
				"                  ('region_name', 'string', 'region_name', 'string'), \n",
				"                  ('insured_value', 'string', 'insured_value', 'double'), \n",
				"                  ('business_type', 'string', 'business_type', 'string'), \n",
				"                  ('earthquake', 'string', 'earth_quake', 'char(1)'), \n",
				"                  ('flood', 'string', 'flood', 'char(1)')]\n",
				"#                  ('update_date', 'string', 'update_date', 'date')]\n",
				"\n",
				"      dyf = dyf.apply_mapping(mappings)\n",
				"      data_frame = dyf.toDF()\n",
				"\n",
				"      data_frame = (data_frame\n",
				"                        .withColumn('file_name',      F.input_file_name())\n",
				"                        .withColumn('year_month_day', F.expr(\"substring(file_name, length(file_name) -11, 8)\"))\n",
				"                        .withColumn('year',           F.expr(\"substring(year_month_day, 1, 4)\"))\n",
				"                        .withColumn('month',          F.expr(\"substring(year_month_day, 5, 2)\"))\n",
				"                        .withColumn('day',            F.expr(\"substring(year_month_day, 7, 2)\"))\n",
				"                        .drop('operation')\n",
				"                        .dropDuplicates()\n",
				"                  )\n",
				"\n",
				"      target_df = data_frame.select([F.col(c) for c in data_frame.columns])\n",
				"      return target_df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"# loading\n",
				"\n",
				"def historical_load(target_df, path):\n",
				"\n",
				"    try:\n",
				"        (target_df.write\n",
				"            .format('delta')\n",
				"            .mode('overwrite') \n",
				"            .partitionBy(['year', 'month', 'day'])\n",
				"            .option(\"overwriteSchema\", \"true\")\n",
				"            .option(\"path\", path)\n",
				"            .save()\n",
				"        )\n",
				"    except:\n",
				"        print(f\"**** Error saving into the bucket {path}\")\n",
				"        sys.exit(-1)\n",
				"        raise\n",
				"        "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"# upsert\n",
				"\n",
				"def delta_load(delta_df, primary_keys, path):\n",
				"\n",
				"    try:\n",
				"        target_df = DeltaTable.forPath(spark, path)\n",
				"    except:\n",
				"        print('**** Target S3 target folder has not found.')\n",
				"        sys.exit(-1)\n",
				"        raise\n",
				"\n",
				"    try:\n",
				"        (target_df.alias('target')\n",
				"                .merge( source    = delta_df.alias('delta'),\n",
				"                        condition = F.expr(dedup_keys_str(primary_keys)))\n",
				"                .whenMatchedUpdateAll()\n",
				"                .whenNotMatchedInsertAll()\n",
				"        ).execute()\n",
				"    except:\n",
				"        print(f\"**** Error upserting into bucket {path}\")\n",
				"        sys.exit(-1)\n",
				"        raise\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"def main(args) -> None:\n",
				"    environment  = args['environment']\n",
				"    s3_bucket    = args['s3_bucket']\n",
				"    start_date   = args['start_date']\n",
				"    final_date   = args['final_date']\n",
				"\n",
				"    ingestion    = 'raw-data'\n",
				"    catalog      = 'glue-catalog'\n",
				"    database     = 'insurance_db'\n",
				"    table_name   = 'policy'\n",
				"    primary_keys = ['policy_id']\n",
				"\n",
				"    full_source_path = s3_bucket + ingestion +'/'+ database +'/'+ table_name + '/full-load/'\n",
				"    cdc_source_path  = s3_bucket + ingestion +'/'+ database +'/'+ table_name + '/cdc-load/'\n",
				"    target_path      = s3_bucket + catalog +'/'+ database +'/'+ table_name + '/'\n",
				"\n",
				"    if not s3_bucket_exists(s3_bucket):\n",
				"        print('**** Bucket name is invalid.')\n",
				"        sys.exit(-1)\n",
				"        raise\n",
				"        \n",
				"    if not start_date:\n",
				"        start_date = (datetime.date.today() - timedelta.days(1)).strftime('%Y-%m-%d')\n",
				"    elif not is_valid_date(start_date):\n",
				"        print('**** Start date is invalid.')\n",
				"        sys.exit(-1)\n",
				"        raise\n",
				"        \n",
				"    if not final_date:\n",
				"        final_date = datetime.date.today().strftime('%Y-%m-%d')\n",
				"    elif not is_valid_date(final_date):\n",
				"        print('**** Final date is invalid.')\n",
				"        sys.exit(-1)\n",
				"        raise\n",
				"\n",
				"\n",
				"    if delta_table_exists(target_path):\n",
				"        source_path = cdc_source_path\n",
				"    else: \n",
				"        source_path = full_source_path\n",
				"        \n",
				"    print('Start date: ', start_date)\n",
				"    print('Final date: ', final_date)\n",
				"    print('Source path:', source_path)\n",
				"    print('Target path:', target_path)\n",
				"\n",
				"\n",
				"    src_df = read_source(source_path, start_date, final_date)\n",
				"    final_df = transform(src_df)\n",
				"    \n",
				"    final_df.show()\n",
				"    \n",
				"    if delta_table_exists(target_path):\n",
				"        print(' >>> Delta loading')\n",
				"        delta_load(final_df, primary_keys, target_path)\n",
				"    else:\n",
				"        print(' >>> Historiccal loading')\n",
				"        historical_load(final_df, target_path)\n",
				"        \n",
				"    delta_df = spark.read.format('delta').load(target_path)\n",
				"    delta_df.select('year_month_day').distinct().show()\n",
				"    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"   \n",
				"args = getResolvedOptions(sys.argv, ['JOB_NAME', 'environment', 's3_bucket', 'start_date', 'final_date'])\n",
				"\n",
				"job = Job(glueContext)\n",
				"job.init(args[\"JOB_NAME\"], args)\n",
				"\n",
				"main(args)\n",
				"\n",
				"job.commit()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "python"
				}
			},
			"outputs": [],
			"source": [
				"%stop_session"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
